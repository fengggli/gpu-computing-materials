

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Changelog &mdash; gpu-computing19-awnn  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Convolution Layer" href="layers/conv.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> gpu-computing19-awnn
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Tensor</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">Tensor and its operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="memory.html">Memory management</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">CUDA Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="mkl.html">MKL notes</a></li>
</ul>
<p class="caption"><span class="caption-text">Layers:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="layers/pool.html">Pooling Layer</a></li>
<li class="toctree-l1"><a class="reference internal" href="layers/conv.html">Convolution Layer</a></li>
</ul>
<p class="caption"><span class="caption-text">Changelog</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Changelog</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#current">Current</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#added">Added</a></li>
<li class="toctree-l3"><a class="reference internal" href="#working-on">Working on</a></li>
<li class="toctree-l3"><a class="reference internal" href="#todo-list">TODO List</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#previous">Previous</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id2">0.4.12</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id3">Added</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id4">0.4.11</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id5">Added</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id6">0.4.10</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id7">Added</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id8">0.4.9</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id9">Added</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id10">0.4.8</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id11">0.4.7</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id12">0.4.6</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id13">Added</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id14">0.4.5</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id15">Added</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id16">0.4.4</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id17">Added</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id18">0.4.3</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id19">Added</a></li>
<li class="toctree-l3"><a class="reference internal" href="#changed">Changed</a></li>
<li class="toctree-l3"><a class="reference internal" href="#others">others</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id20">0.4.2</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id21">Added</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id22">&lt; 0.4.1</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">gpu-computing19-awnn</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Changelog</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/changelog.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="changelog">
<span id="id1"></span><h1>Changelog<a class="headerlink" href="#changelog" title="Permalink to this headline">¶</a></h1>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>Code is maintained in <a class="reference external" href="https://github.com/fengggli/gpu-computing-materials/">https://github.com/fengggli/gpu-computing-materials/</a></p>
<p class="last">Ask Feng for access.</p>
</div>
<div class="section" id="current">
<h2>Current<a class="headerlink" href="#current" title="Permalink to this headline">¶</a></h2>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Date:</th><td class="field-body">2019-10-22</td>
</tr>
</tbody>
</table>
<div class="section" id="added">
<h3>Added<a class="headerlink" href="#added" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li>Explained why AWNN is slower than Intel-Caffe in Stampede2 SKX node Gibson (also SKX with avx512)
- performance analysis results using intel vtune see (<a class="reference external" href="https://github.com/fengggli/gpu-computing-materials/issues/57">https://github.com/fengggli/gpu-computing-materials/issues/57</a>)
- AWNN still has worse single-threaded performance, most of the elapsed time is spent on im2col and col2im, since they are not currently vectorized.
- intel-caffe uses mkldnn  JIT avx code generation to accelerate operations like convolution/pooling.
- A SC18 paper describes some of the optimizations used in MKL-DNN(e.g. vectorization, cache/register blocking, loop reordering, kernel streaming, software prefetching, layer fusion, etc:  <a class="reference external" href="https://dl.acm.org/citation.cfm?id=3291744">https://dl.acm.org/citation.cfm?id=3291744</a>)</li>
<li>Followed several suggestions from intel performance guide, improved single-thread forward/backward time from 540 to 380ms(<a class="reference external" href="https://github.com/fengggli/gpu-computing-materials/issues/57#issuecomment-540705655">https://github.com/fengggli/gpu-computing-materials/issues/57#issuecomment-540705655</a>).</li>
<li>We can add those optimization implemented in MKLDNN, (e.g. vectorization of im2col/col2im). But such optimizations are not urgent.</li>
<li>Some literature on pipeline parallelism (<a class="reference external" href="https://fengggli.github.io/ResearchDocs/topics/pipeline/pipeline.html#pipeline">https://fengggli.github.io/ResearchDocs/topics/pipeline/pipeline.html#pipeline</a>), it’s a form of model parallelism.</li>
</ol>
</div>
<div class="section" id="working-on">
<h3>Working on<a class="headerlink" href="#working-on" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li>Original communication optimal algorithm only considers forward pass of a direct convolution operation. It gives guidelines how to decide on best blocking size/ loop orders in a direction convolution, for different input settings(e.g. input image size, filter sizes, stride size, etc).</li>
<li>How to extend it to other components in a neural-net?</li>
<li>How to decide the balance of data/model parallelism.</li>
</ol>
</div>
<div class="section" id="todo-list">
<h3>TODO List<a class="headerlink" href="#todo-list" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>Use fine-grained lock to reduce contention.</li>
<li>Theoretical model.</li>
</ul>
</div>
</div>
</div>
<div class="section" id="previous">
<h1>Previous<a class="headerlink" href="#previous" title="Permalink to this headline">¶</a></h1>
<div class="section" id="id2">
<h2>0.4.12<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Date:</th><td class="field-body">2019-10-07</td>
</tr>
</tbody>
</table>
<div class="section" id="id3">
<h3>Added<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>Performance comparision with Intel-caffe in skx and knl nodes and corresponding analysis.<ul>
<li>Performance of intel-caffe is x3.9 faster than awnn in stampede skx(<a class="reference external" href="https://github.com/fengggli/gpu-computing-materials/issues/54#issuecomment-537741399">https://github.com/fengggli/gpu-computing-materials/issues/54#issuecomment-537741399</a>), not consistent with the sievert results.</li>
<li>Now I am able to build caffe using preloaded dependencies in stampede2. Need to profile to understand the inconsistent performance in stampedede2.</li>
<li>Also need to do same set of experiments in gibson.</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="id4">
<h2>0.4.11<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<p>:Data 2019-09-26</p>
<div class="section" id="id5">
<h3>Added<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li>Add worker threads support, details are at: <a class="reference external" href="https://github.com/fengggli/gpu-computing-materials/issues/54">https://github.com/fengggli/gpu-computing-materials/issues/54</a></li>
<li>reorganize code-structure, so that:<ul>
<li>each type of layer is now associated with a “layer_setup” function, which can infer the size of output tensor and working memory based on the layer below it.</li>
<li>all working memory and middle-layer output memory are preallocated during the “set_up” phase, instead allocated/free during forward/backward</li>
<li>improved implementations of layers like fc/relu/pool to reduce extra memory copies.</li>
<li>x1.77 speedup, using float32(in sievert).</li>
</ul>
</li>
</ol>
</div>
</div>
<div class="section" id="id6">
<h2>0.4.10<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<p>:Data 2019-08-23</p>
<div class="section" id="id7">
<h3>Added<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li>model, extended resnet with 3 stages:
* previous simple model: <a class="reference external" href="http://ethereon.github.io/netscope/#/gist/64b013d6fee840473edc1a9a444e22ca">http://ethereon.github.io/netscope/#/gist/64b013d6fee840473edc1a9a444e22ca</a>
* new 14-layer model: <a class="reference external" href="http://ethereon.github.io/netscope/#/gist/b14a68b31b3973c68b38dfc2f73d2d10">http://ethereon.github.io/netscope/#/gist/b14a68b31b3973c68b38dfc2f73d2d10</a></li>
</ol>
</div>
</div>
<div class="section" id="id8">
<h2>0.4.9<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h2>
<p>:Data 2019-06-27</p>
<div class="section" id="id9">
<h3>Added<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li>Adding downsampling in the beginning of stage 3,4,5, more details see <a class="reference external" href="https://github.com/fengggli/gpu-computing-materials/issues/51">https://github.com/fengggli/gpu-computing-materials/issues/51</a>, ignoring the boundries.</li>
<li>Residual blocks using with downsampling support and its tests.</li>
<li>Add resnet14, made of 3 stages, each stage containing 2 residual blocks.</li>
</ol>
</div>
</div>
<div class="section" id="id10">
<h2>0.4.8<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h2>
<p>:Data 2019-05-12</p>
<ul class="simple">
<li>Add nnpack support, resnet can use nnpack backend for the convolution operations(<a class="reference external" href="https://github.com/fengggli/gpu-computing-materials/pull/41">https://github.com/fengggli/gpu-computing-materials/pull/41</a>)</li>
<li>Initial implementation of convolution is slow due to explict transpose and memory copies. (<a class="reference external" href="https://github.com/fengggli/gpu-computing-materials/pull/41#issuecomment-486513801">https://github.com/fengggli/gpu-computing-materials/pull/41#issuecomment-486513801</a>), we did performance analysis and improvement for the convolution layer.</li>
<li>Add per-image convolution like in Caffe(<a class="reference external" href="https://github.com/fengggli/gpu-computing-materials/pull/49">https://github.com/fengggli/gpu-computing-materials/pull/49</a>).</li>
<li>There is also a comparision of AWNN vs caffe in the case of (1)NNPACK or (2)per-img im2col+openblas gemm when different batch sizes are used (<a class="reference external" href="https://github.com/fengggli/gpu-computing-materials/pull/49#issuecomment-490657411">https://github.com/fengggli/gpu-computing-materials/pull/49#issuecomment-490657411</a>): Our implementation is slightly faster than Caffe when using openblas gemm; nnpack in caffe patch doesn’t provide backward implementation, I can add it though.</li>
</ul>
</div>
<div class="section" id="id11">
<h2>0.4.7<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h2>
<p>:Data 2019-04-22</p>
<ul class="simple">
<li>Simplified resnet(<a class="reference external" href="https://github.com/fengggli/gpu-computing-materials/pull/38">https://github.com/fengggli/gpu-computing-materials/pull/38</a>)</li>
<li>Fix memory leaks, and some obvious optimization.</li>
<li>Initializer (kaiming initialization)</li>
</ul>
</div>
<div class="section" id="id12">
<h2>0.4.6<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h2>
<p>:Data 2019-04-15</p>
<div class="section" id="id13">
<h3>Added<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>residual block and simple resnet. See <a class="reference external" href="https://github.com/fengggli/gpu-computing-materials/pull/37">https://github.com/fengggli/gpu-computing-materials/pull/37</a>.</li>
</ul>
</div>
</div>
<div class="section" id="id14">
<h2>0.4.5<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h2>
<p>:Date 2019-04-10</p>
<div class="section" id="id15">
<h3>Added<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>utils for debug use (tensor mean/std, etc)</li>
<li>fixed several bugs</li>
<li>utils to report statistics during training(loss, train/val accuracy).</li>
<li>results of mlp is in <a class="reference external" href="https://github.com/fengggli/gpu-computing-materials/pull/27/">https://github.com/fengggli/gpu-computing-materials/pull/27/</a></li>
</ul>
</div>
</div>
<div class="section" id="id16">
<h2>0.4.4<a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h2>
<p>:Date 2019-04-08</p>
<div class="section" id="id17">
<h3>Added<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li>cifar Data loader:</li>
</ol>
<blockquote>
<div><ul class="simple">
<li>Use data/cifar10/get_cifar10.sh to download data.</li>
<li>preprocess: normailzed, and with channel mean substracted.</li>
<li>train/validation split</li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="2">
<li>Solver(main for loop):</li>
</ol>
<blockquote>
<div><ul class="simple">
<li>feed batches from loader, forward/backward and gradient updates(test/test_net_mlp_cifar)</li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="2">
<li>Weight init</li>
</ol>
<blockquote>
<div><ul class="simple">
<li>Kaiming init and weight-scale based init.</li>
<li>Extract this part to utils/ since we use distribution from stl.</li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="3">
<li>Doc</li>
</ol>
<blockquote>
<div><ul class="simple">
<li>Added the network memory allocation figure.</li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="4">
<li>Cuda</li>
</ol>
<blockquote>
<div><ul class="simple">
<li>naiive CUDA pooling layer, set USE_CUDA=on to enable</li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="id18">
<h2>0.4.3<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h2>
<p>:Date 2019-04-01</p>
<p>See (<a class="reference external" href="https://github.com/fengggli/gpu-computing-materials/pull/19">https://github.com/fengggli/gpu-computing-materials/pull/19</a>)</p>
<div class="section" id="id19">
<h3>Added<a class="headerlink" href="#id19" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>a fc_relu sandwich layer</li>
<li>weight initialization (currently only linspace is used)</li>
<li>macro: tensor_for_each_entry in tensor.h</li>
<li>net-mlp:<ul>
<li>inference-only forward - mlp_forward</li>
<li>loss function to update the gradients mlp_loss</li>
<li>forward compared with numpy version</li>
<li>backward checked with numerical results</li>
<li>regulizer is  added</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="changed">
<h3>Changed<a class="headerlink" href="#changed" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>changed the layer cache, now each layer has a lcache_t, which can be assessed as a stack using lcache_push, and lcache_pop. See docs/source/memory.rst for more details</li>
</ul>
</div>
<div class="section" id="others">
<h3>others<a class="headerlink" href="#others" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>clangformat using google style</li>
</ul>
</div>
</div>
<div class="section" id="id20">
<h2>0.4.2<a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h2>
<p>:Date 2019-03-30</p>
<div class="section" id="id21">
<h3>Added<a class="headerlink" href="#id21" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li>Layers:</li>
</ol>
<blockquote>
<div><ul class="simple">
<li>fully-connected</li>
<li>global avg pool.</li>
<li>relu</li>
<li>softmax</li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="2">
<li>Data structure</li>
</ol>
<blockquote>
<div><ul class="simple">
<li>The param_t uses linux-kernel style linked list, which can be also used to construct other basic data structures like stack/queue.</li>
<li>currently it’s used to manage all learnable params of fc layers.</li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="id22">
<h2>&lt; 0.4.1<a class="headerlink" href="#id22" title="Permalink to this headline">¶</a></h2>
<p>see dl-docs for changelog prior to 0.4.1</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="layers/conv.html" class="btn btn-neutral float-left" title="Convolution Layer" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Feng Li, Christerpher Goebel,Yuankun Fu

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>